# -*- coding: utf-8 -*-
"""Data-science.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eB8SiIRvFdMmwh5NbA7iw95oD89-2cOh

## First Step
### Question
#### Can you estimate an abalone's age using its physical characteristics?
#### Investigate which variables are better predictors of age for abalones.
#### estimate an abalone's age using ML/DL

## 2nd step
### Data acquisition

#### https://www.kaggle.com/datasets/devzohaib/estimating-the-age-of-abalone-at-a-seafood-farm

## 3rd step and 4th step
### Extract, transform, load and Data wrangling
#### importing required  pakages
"""

import pandas as pd
import matplotlib.pyplot as plt

"""### Extracting data"""

df = pd.read_csv("abalone.csv")
df

"""### Data wrangling"""

df.describe()

df.shape

df.drop_duplicates(inplace=True)

df.shape

df.isnull().sum()

"""## 5th step
### Data visualization
"""

# Specify figure size
plt.figure(figsize=(16, 3))

# Subplot1
plt.subplot(1, 5, 1)
plt.scatter(x=df['sex'], y=df['age'], edgecolors='white')
plt.title('Age vs Sex')
plt.xlabel('Rings')
plt.ylabel('Age')

# Subplot2
plt.subplot(1, 5, 2)
plt.scatter(x=df['length'], y=df['age'], edgecolors='white')
plt.title('Age vs Length')
plt.xlabel('length')

# Subplot3
plt.subplot(1, 5, 3)
plt.scatter(x=df['diameter'], y=df['age'], edgecolors='white')
plt.title('Age vs diameter')
plt.xlabel('diameter')

# Subplot4
plt.subplot(1, 5, 4)
plt.scatter(x=df['height'], y=df['age'], edgecolors='white')
plt.title('Age vs height')
plt.xlabel('height')

# Subplot5
plt.subplot(1, 5, 5)
plt.scatter(x=df['whole_wt'], y=df['age'], edgecolors='white')
plt.title('Age vs whole_wt')
plt.xlabel('whole_wt')


plt.show()

# Specify figure size
plt.figure(figsize=(16, 3))

# Subplot1
plt.subplot(1, 4, 1)
plt.scatter(x=df['shucked_wt'], y=df['age'], edgecolors='white')
plt.title('Age vs shucked_wt')
plt.xlabel('shucked_wt')
plt.ylabel('Age')

# Subplot2
plt.subplot(1, 4, 2)
plt.scatter(x=df['rings'], y=df['age'], edgecolors='white')
plt.title('Age vs rings')
plt.xlabel('rings')


# Subplot3
plt.subplot(1, 4, 3)
plt.scatter(x=df['shell_wt'], y=df['age'], edgecolors='white')
plt.title('Age vs shell_wt')
plt.xlabel('shell_wt')

# Subplot4
plt.subplot(1, 4, 4)
plt.scatter(x=df['viscera_wt'], y=df['age'], edgecolors='white')
plt.title('Age vs viscera_wt')
plt.xlabel('viscera_wt')

plt.show()

"""## 6th step
### choose
#### As from visulaization of data we can see Ring and Age has linear relation so we will use Linear regression that will give us best output and is best when data has relation so  Rings variable is better predictor of age for abalones.

## 7th step
### build

###  Linear regression Model for predict the age of abalone
"""

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X = df[['rings']].values
y= df[['age']].values

"""#### spliting data into tranning and testing data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,
                            test_size=0.2)

X_train.shape, y_test.shape

# initialize instance from LinearRegression as regressor

regressor = LinearRegression()
regressor.fit(X_train, y_train)

print("Training complete.")



"""### Testing modle and Accuracy"""

# Evaluate model performance

from sklearn.metrics import mean_squared_error
cof = regressor.score(X_test, y_test)

# Mean Squared Error
y_pred = regressor.predict(X_test)
MSE = mean_squared_error(y_pred, y_test)

print(f'Accuracy of modle:{cof*100 }\nMean Squared Error:{MSE}')

# Find the model Coefficient and Intercept
inter = regressor.intercept_
coef = regressor.coef_

# construct the model formula
y_hat = inter + coef*X

plt.figure(figsize=(7, 4))
plt.scatter(X, y)
plt.plot(X, y_hat, c='orange')
plt.legend(['data point', 'model'])
plt.xlabel('No. of Rings')
plt.ylabel("Age")

"""### validating modle"""

# Predict Age with 9 Rings of abalone
pred = regressor.predict(np.array(9, ndmin=2))
print(f"No. of Rings: 9\nPredicted Age is: {round(pred[0][0],2)} ")

# final result
plt.figure(figsize=(7,4))
plt.scatter(X,y)
plt.scatter(9,pred,c='red')
plt.plot(X,y_hat,c='orange')
plt.legend(['Data','test data with 9 rings','model'])
plt.show()

# Predict Age with 32 Rings of abalone

pred = regressor.predict(np.array(32,ndmin=2))
print(f"No. of Rings: 32\nPredicted Age is: {round(pred[0][0],2)} ")

# final result
plt.figure(figsize=(7,4))
plt.scatter(X,y)
plt.scatter(32,pred,c='red')
plt.plot(X,y_hat,c='orange')
plt.legend(['Data','test data with 32 rings','model'])
plt.show()

"""# Using KNN"""
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report


def load_and_preprocess_data():
    # Load the abalone dataset with the correct column names
    data = pd.read_csv('abalone.csv')

    # Convert Sex to numeric using one-hot encoding
    data = pd.get_dummies(data, columns=['sex'])

    # Split features and target
    X = data.drop(['rings', 'age'], axis=1)  # Remove both rings and age columns
    y = data['rings']  # Use rings as target variable

    # Split into 70% training and 30% testing
    train_size = int(0.7 * len(X))

    X_train = X[:train_size]
    X_test = X[train_size:]
    y_train = y[:train_size]
    y_test = y[train_size:]

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test


def tune_k(X_train, X_test, y_train, y_test, k_range=range(1, 31, 2)):
    train_scores = []
    test_scores = []

    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train, y_train)

        train_pred = knn.predict(X_train)
        test_pred = knn.predict(X_test)

        train_scores.append(accuracy_score(y_train, train_pred))
        test_scores.append(accuracy_score(y_test, test_pred))

    # Find best k
    best_k = k_range[np.argmax(test_scores)]

    return k_range, train_scores, test_scores, best_k


def calculate_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    # Using weighted average since this is a multi-class problem
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')

    return accuracy, precision, recall, f1


def plot_k_accuracy(k_range, train_scores, test_scores, best_k):
    plt.figure(figsize=(10, 6))
    plt.plot(k_range, train_scores, label='Training Accuracy')
    plt.plot(k_range, test_scores, label='Testing Accuracy')
    plt.xlabel('K Value')
    plt.ylabel('Accuracy')
    plt.title('KNN: K Value vs. Accuracy')
    plt.legend()
    plt.grid(True)
    plt.axvline(x=best_k, color='r', linestyle='--', label=f'Best K={best_k}')
    plt.savefig('knn_accuracy.png')
    plt.close()


def main():
    try:
        # Load and preprocess data
        print("Loading and preprocessing data...")
        X_train, X_test, y_train, y_test = load_and_preprocess_data()

        # Tune K and get accuracy curves
        print("Tuning K parameter...")
        k_range, train_scores, test_scores, best_k = tune_k(X_train, X_test, y_train, y_test)

        # Train final model with best K
        print(f"Training final model with K={best_k}...")
        final_model = KNeighborsClassifier(n_neighbors=best_k)
        final_model.fit(X_train, y_train)

        # Get predictions
        train_pred = final_model.predict(X_train)
        test_pred = final_model.predict(X_test)

        # Calculate metrics
        train_metrics = calculate_metrics(y_train, train_pred)
        test_metrics = calculate_metrics(y_test, test_pred)

        # Plot results
        plot_k_accuracy(k_range, train_scores, test_scores, best_k)

        # Print results
        print(f"\nBest K value: {best_k}")
        print("\nTraining Set Metrics:")
        print(f"Accuracy: {train_metrics[0]:.4f}")
        print(f"Precision: {train_metrics[1]:.4f}")
        print(f"Recall: {train_metrics[2]:.4f}")
        print(f"F1-Score: {train_metrics[3]:.4f}")

        print("\nTest Set Metrics:")
        print(f"Accuracy: {test_metrics[0]:.4f}")
        print(f"Precision: {test_metrics[1]:.4f}")
        print(f"Recall: {test_metrics[2]:.4f}")
        print(f"F1-Score: {test_metrics[3]:.4f}")

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        print("Please check your data file format and try again.")


if __name__ == "__main__":
    main()